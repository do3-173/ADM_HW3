{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Engine\n",
    "\n",
    "Now, we want to create two different Search Engines that, given as input a query, return the courses that match the query.\n",
    "\n",
    "### 2.0 Preprocessing \n",
    "\n",
    "### 2.0.0)  Preprocessing the text\n",
    "\n",
    "First, you must pre-process all the information collected for each MSc by:\n",
    "\n",
    "1. Removing stopwords\n",
    "2. Removing punctuation\n",
    "3. Stemming\n",
    "4. Anything else you think it's needed\n",
    "   \n",
    "For this purpose, you can use the [`nltk library](https://www.nltk.org/).\n",
    "\n",
    "### 2.0.1) Preprocessing the fees column\n",
    "\n",
    "Moreover, we want the field ```fees``` to collect numeric information. As you will see, you scraped textual information for this attribute in the dataset: sketch whatever method you need (using regex, for example, to find currency symbol) to collect information and, in case of multiple information, retrieve only the highest fees. Finally, once you have collected numerical information, you likely will have different currencies: this can be chaotic, so let chatGPT guide you in the choice and deployment of an API to convert this column to a common currency of your choice (it can be USD, EUR or whatever you want). Ultimately, you will have a ```float``` column renamed ```fees (CHOSEN COMMON CURRENCY)```.\n",
    "\n",
    "### 2.1. Conjunctive query\n",
    "For the first version of the search engine, we narrowed our interest to the __description__ of each course. It means that you will evaluate queries only concerning the course's description.\n",
    "\n",
    "### 2.1.1) Create your index!\n",
    "\n",
    "Before building the index, \n",
    "* Create a file named `vocabulary`, in the format you prefer, that maps each word to an integer (`term_id`).\n",
    "\n",
    "Then, the first brick of your homework is to create the Inverted Index. It will be a dictionary in this format:\n",
    "\n",
    "```\n",
    "{\n",
    "term_id_1:[document_1, document_2, document_4],\n",
    "term_id_2:[document_1, document_3, document_5, document_6],\n",
    "...}\n",
    "```\n",
    "where _document\\_i_ is the *id* of a document that contains that specific word.\n",
    "\n",
    "__Hint:__ Since you do not want to compute the inverted index every time you use the Search Engine, it is worth thinking about storing it in a separate file and loading it in memory when needed.\n",
    "\n",
    "#### 2.1.2) Execute the query\n",
    "Given a query input by the user, for example:\n",
    "\n",
    "```\n",
    "advanced knowledge\n",
    "```\n",
    "\n",
    "The Search Engine is supposed to return a list of documents.\n",
    "\n",
    "##### What documents do we want?\n",
    "Since we are dealing with conjunctive queries (AND), each returned document should contain all the words in the query.\n",
    "The final output of the query must return, if present, the following information for each of the selected documents:\n",
    "\n",
    "* `courseName`\n",
    "* `universityName`\n",
    "* `description`\n",
    "* `URL`\n",
    "\n",
    "__Example Output__ for ```advanced knowledge```: (please note that our examples are made on a small batch of the full dataset)\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"img/output1.png\" width = 1000>\n",
    "</p>\n",
    "\n",
    "If everything works well in this step, you can go to the next point and make your Search Engine more complex and better at answering queries.\n",
    "\n",
    "\n",
    "### 2.2) Conjunctive query & Ranking score\n",
    "\n",
    "For the second search engine, given a query, we want to get the *top-k* (the choice of *k* it's up to you!) documents related to the query. In particular:\n",
    "\n",
    "* Find all the documents that contain all the words in the query.\n",
    "* Sort them by their similarity with the query.\n",
    "* Return in output *k* documents, or all the documents with non-zero similarity with the query when the results are less than _k_. You __must__ use a heap data structure (you can use Python libraries) for maintaining the *top-k* documents.\n",
    "\n",
    "To solve this task, you must use the *tfIdf* score and the _Cosine similarity_. The field to consider is still the `description`. Let's see how.\n",
    "\n",
    "\n",
    "#### 2.2.1) Inverted index\n",
    "Your second Inverted Index must be of this format:\n",
    "\n",
    "```\n",
    "{\n",
    "term_id_1:[(document1, tfIdf_{term,document1}), (document2, tfIdf_{term,document2}), (document4, tfIdf_{term,document4}), ...],\n",
    "term_id_2:[(document1, tfIdf_{term,document1}), (document3, tfIdf_{term,document3}), (document5, tfIdf_{term,document5}), (document6, tfIdf_{term,document6}), ...],\n",
    "...}\n",
    "```\n",
    "\n",
    "Practically, for each word, you want the list of documents in which it is contained and the relative *tfIdf* score.\n",
    "\n",
    "__Tip__: *TfIdf* values are invariant for the query. Due to this reason, you can precalculate and store them accordingly.\n",
    "\n",
    "#### 2.2.2) Execute the query\n",
    "\n",
    "In this new setting, given a query, you get the proper documents (i.e., those containing all the query's words) and sort them according to their similarity to the query. For this purpose, as the scoring function, we will use the Cosine Similarity concerning the *tfIdf* representations of the documents.\n",
    "\n",
    "Given a query input by the user, for example:\n",
    "```\n",
    "advanced knowledge\n",
    "```\n",
    "The search engine is supposed to return a list of documents, __ranked__ by their Cosine Similarity to the query entered in the input.\n",
    "\n",
    "More precisely, the output must contain:\n",
    "* `courseName`\n",
    "* `universityName`\n",
    "* `description`\n",
    "* `URL`\n",
    "* The similarity score of the documents with respect to the query (float value between 0 and 1)\n",
    "  \n",
    "__Example Output__ for ```advanced knowledge```:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"img/output2.png\" width = 1000>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the TSV data\n",
    "df = pd.read_csv(\n",
    "    \"TSV/course_1.tsv\", sep=\"\\t\", index_col=False\n",
    ")\n",
    "\n",
    "for i in range(2, 6001):\n",
    "    try:\n",
    "        df1 = pd.read_csv(\n",
    "            \"TSV/course_\" + str(i) + \".tsv\",\n",
    "            sep=\"\\t\",\n",
    "            index_col=False,\n",
    "        )\n",
    "        df1.index += i - 1\n",
    "        df = pd.concat([df, df1])\n",
    "    except Exception as e:\n",
    "        print(i)\n",
    "        print(\"Error: \", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/petraudovicic/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Removing stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def stopless(text):\n",
    "    if isinstance(text, str):\n",
    "        words = word_tokenize(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        return \" \".join(filtered_words)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "df = df.applymap(stopless)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/petraudovicic/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "#removing punctuation\n",
    "nltk.download('punkt')\n",
    "def punct(text):\n",
    "    if isinstance(text, str):\n",
    "        words = word_tokenize(text)\n",
    "        filtered_words = [word for word in words if word.lower() not in string.punctuation]\n",
    "        return \" \".join(filtered_words)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "df = df.applymap(punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/petraudovicic/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "ps = PorterStemmer()\n",
    "def stem(text):\n",
    "    if isinstance(text, str):\n",
    "        words = word_tokenize(text)\n",
    "        stemmed_words = [ps.stem(word) for word in words]\n",
    "        return \" \".join(stemmed_words)\n",
    "    else:\n",
    "        return text\n",
    "df = df.applymap(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fees</th>\n",
       "      <th>converted_fees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uk student full time £9450 2022/2023 academ ye...</td>\n",
       "      <td>18880.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>tuition fee uk student start full-tim studi 20...</td>\n",
       "      <td>11797.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>uk student integr pre-mast cours £10,500 msc c...</td>\n",
       "      <td>12015.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>full time home award fee £11500 home modul fee...</td>\n",
       "      <td>13159.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>€11,000</td>\n",
       "      <td>11000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5988</th>\n",
       "      <td>full time £18,000 part time £9,000</td>\n",
       "      <td>20597.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5989</th>\n",
       "      <td>full time £18,000 part time £9,000</td>\n",
       "      <td>20597.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5993</th>\n",
       "      <td>full time £14,100 part time £7,050</td>\n",
       "      <td>16134.630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5994</th>\n",
       "      <td>full time £18,000 part time £9,000</td>\n",
       "      <td>20597.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>full time £18,000 part time £9,000</td>\n",
       "      <td>20597.400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1228 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   fees  converted_fees\n",
       "0     uk student full time £9450 2022/2023 academ ye...       18880.950\n",
       "29    tuition fee uk student start full-tim studi 20...       11797.733\n",
       "49    uk student integr pre-mast cours £10,500 msc c...       12015.150\n",
       "50    full time home award fee £11500 home modul fee...       13159.450\n",
       "51                                              €11,000       11000.000\n",
       "...                                                 ...             ...\n",
       "5988                 full time £18,000 part time £9,000       20597.400\n",
       "5989                 full time £18,000 part time £9,000       20597.400\n",
       "5993                 full time £14,100 part time £7,050       16134.630\n",
       "5994                 full time £18,000 part time £9,000       20597.400\n",
       "5997                 full time £18,000 part time £9,000       20597.400\n",
       "\n",
       "[1228 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def convert_to_numeric(value):\n",
    "    value = re.sub(r'eur|sek|chf|gbp|rmb|jpy|qr|[£€]', '', value)\n",
    "    # Remove commas from the numeric part (e.g., for values like '1,000')\n",
    "    value = value.replace(',', '')\n",
    "    value=value.replace(' ', '')\n",
    "    return float(value)\n",
    "def find_fees(text):\n",
    "    if isinstance(text,str):\n",
    "        pattern = r'\\b\\d{4}/\\d{4}\\b'\n",
    "        text = re.sub(pattern, '', text)\n",
    "        pattern = r'\\b\\d{4}/\\d{2}\\b'\n",
    "        text = re.sub(pattern, '', text)\n",
    "        # Define the regular expression pattern for currency symbols and values\n",
    "        pattern = r'((eur|sek|chf|gbp|rmb|jpy|qr|[£€])\\s?\\d+(?:[.,\\s]\\d{3})*(?:[.,]\\d{2})?|\\d+(?:[.,\\s]\\d{3})*(?:[.,]\\d{2})?\\s?(eur|sek|chf|gbp|rmb|jpy|qr|[£€]))'\n",
    "        # Find all matches in the text\n",
    "        matches = re.findall(pattern, text)\n",
    "        # Convert values to numeric format and filter out non-euro values\n",
    "        numeric_values = []\n",
    "        exchange_rates = {'SEK': 0.08588, 'GBP': 1.1443, 'CHF': 1.03708, 'JPY': 0.00618, 'QR': 0.25672, \"RMB\": 0.12892}\n",
    "        for value in matches:\n",
    "            value_numeric = convert_to_numeric(value[0])\n",
    "            if (('€' or 'eur') in value):\n",
    "                numeric_values.append(value_numeric)\n",
    "            elif 'sek' in value and exchange_rates is not None and 'SEK' in exchange_rates:\n",
    "                numeric_values.append(value_numeric * exchange_rates['SEK'])\n",
    "            elif ('£' or 'gbp') in value and exchange_rates is not None and 'GBP' in exchange_rates:\n",
    "                numeric_values.append(value_numeric * exchange_rates['GBP'])\n",
    "            elif 'chf' in value and exchange_rates is not None and 'CHF' in exchange_rates:\n",
    "                numeric_values.append(value_numeric * exchange_rates['CHF'])\n",
    "            elif 'jpy' in value and exchange_rates is not None and 'JPY' in exchange_rates:\n",
    "                numeric_values.append(value_numeric * exchange_rates['JPY'])\n",
    "            elif 'qr' in value and exchange_rates is not None and 'QR' in exchange_rates:\n",
    "                numeric_values.append(value_numeric * exchange_rates['QR'])\n",
    "            else:\n",
    "                if ('qr' in value and exchange_rates is not None and 'QR' in exchange_rates):\n",
    "                    numeric_values.append(convert_to_numeric(value) * exchange_rates['RMB'])\n",
    "\n",
    "        if not numeric_values:\n",
    "            return None  # No euro values found\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "    # Find the largest value\n",
    "    max_value = max(numeric_values)\n",
    "\n",
    "    return max_value\n",
    "df['converted_fees'] = df['fees'].apply(find_fees)\n",
    "filtered_df = df.dropna(subset=['converted_fees'])[['fees','converted_fees']]\n",
    "filtered_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'choos', 2: 'hert', 3: 'industri', 4: 'accredit', 5: 'british', 6: 'comput', 7: 'societi', 8: 'bc', 9: 'enabl', 10: 'graduat', 11: 'regist', 12: 'charter', 13: 'institut', 14: 'professionalwork', 15: 'placement', 16: 'opportun', 17: 'option', 18: 'oneyear', 19: 'previou', 20: 'work', 21: 'organis', 22: 'includ', 23: 'hewlett', 24: 'packard', 25: 'bt', 26: 'vodafoneemploy', 27: 'prospect', 28: 'busi', 29: 'analyst', 30: 'hr', 31: 'system', 32: 'manag', 33: 'googl', 34: 'parker', 35: 'hannifin', 36: 'kellkooabout', 37: 'courseon', 38: 'rang', 39: 'degre', 40: 'taught', 41: 'master', 42: 'programm', 43: 'school', 44: 'physic', 45: 'engin', 46: 'scienc', 47: 'cours', 48: 'especi', 49: 'design', 50: 'numer', 51: 'subject', 52: 'mostli', 53: 'separ', 54: 'intens', 55: 'cover', 56: 'broad', 57: 'key', 58: 'principl', 59: 'techniqu', 60: 'scienc', 61: 'join', 62: 'fight', 63: 'malici', 64: 'program', 65: 'cybercrim', 66: 'cyber', 67: 'secur', 68: 'msc', 69: 'give', 70: 'skill', 71: 'need', 72: 'becom', 73: 'digit', 74: 'forens', 75: 'profession', 76: '', 77: 'intercept', 78: 'collect', 79: 'analys', 80: 'report', 81: 'event', 82: 'cyberspac', 83: 'mediaput', 84: 'front', 85: 'line', 86: 'staffordshir', 87: 'univers', 88: 'specialis', 89: 'laboratori', 90: 'isol', 91: 'network', 92: 'vital', 93: 'experi', 94: 'current', 95: 'threat', 96: 'importantli', 97: 'protect', 98: 'neutralis', 99: 'msc', 100: 'excit', 101: 'onecalendaryear', 102: 'aim', 103: 'excel', 104: 'student', 105: 'keen', 106: 'deepen', 107: 'exist', 108: 'knowledg', 109: 'expect', 110: 'reward', 111: 'career', 112: 'profess', 113: 'core', 114: 'compon', 115: 'also', 116: 'suitabl', 117: 'prepar', 118: 'phd', 119: 'studi', 120: '', 121: 'regist', 122: 'interest', 123: 'studi', 124: 'uclth', 125: 'geospati', 126: 'build', 127: 'inform', 128: 'model', 129: 'teach', 130: 'theori', 131: 'tool', 132: 'data', 133: 'acquisit', 134: 'process', 135: 'analysi', 136: 'engag', 137: 'academ', 138: 'partner', 139: 'cuttingedg', 140: 'research', 141: 'seminar', 142: 'dissertationabout', 143: 'degreeth', 144: 'specialist', 145: 'bim', 146: 'educ', 147: 'geometr', 148: 'semant', 149: 'aspect', 150: 'integr', 151: 'bimrel', 152: 'survey', 153: 'd', 154: 'realiti', 155: 'captur', 156: 'gener', 157: 'overview', 158: 'technolog', 159: 'relat', 160: 'map', 161: 'geoinform', 162: 'model', 163: 'interest', 164: 'earth', 165: 'miner', 166: 'extract', 167: 'geophys', 168: 'offer', 169: 'plenti', 170: 'scope', 171: 'get', 172: 'involv', 173: 'expertis', 174: 'requir', 175: 'explor', 176: 'differ', 177: 'type', 178: 'terrain', 179: 'discov', 180: 'newli', 181: 'evolv', 182: 'landscap', 183: 'assist', 184: 'compani', 185: 'latest', 186: 'method', 187: 'variou', 188: 'surfac', 189: 'layer', 190: 'new', 191: 'base', 192: 'solid', 193: 'learn', 194: 'retriev', 195: 'understand', 196: 'risk', 197: 'potenti', 198: 'issu', 199: 'difficult', 200: 'reach', 201: 'place', 202: 'develop', 203: 'signal', 204: 'use', 205: 'advanc', 206: 'sensor', 207: 'set', 208: 'alert', 209: 'monitor', 210: 'specif', 211: 'area', 212: 'normal', 213: 'clever', 214: 'widen', 215: 'consider', 216: 'provid', 217: 'sophist', 218: 'alongsid', 219: 'tradit', 220: 'discoveri', 221: 'apart', 222: 'load', 223: 'bear', 224: 'natur', 225: 'movement', 226: 'may', 227: 'affect', 228: 'seismic', 229: 'look', 230: 'move', 231: 'time', 232: 'earthquak', 233: 'chemic', 234: 'reaction', 235: 'beneath', 236: 'earth', 237: 'geoscienc', 238: 'researchfocus', 239: 'strong', 240: 'background', 241: 'public', 242: 'privat', 243: 'sector', 244: 'subjectspecif', 245: 'researchact', 246: 'staff', 247: 'extend', 248: 'project', 249: 'choic', 250: 'keel', 251: 'ukbas', 252: 'extern', 253: 'oversea'}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from functools import reduce\n",
    "vocabulary_alt = Counter((reduce(lambda x, y: x + y, str(df.description.values))).split()).keys()\n",
    "index = {}\n",
    "unique_id = 1\n",
    "for word in list(vocabulary_alt):\n",
    "  index[unique_id] = word\n",
    "  unique_id+=1\n",
    "#removing characters\n",
    "def remove_non_alpha(input_string):\n",
    "    return ''.join(char for char in input_string if char.isalpha())\n",
    "for i in index.keys():\n",
    "   index[i] = remove_non_alpha(str(index[i]))\n",
    "print(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(lowercase=False, tokenizer=<function <lambda> at 0x1250f3b00>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petraudovicic/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tfidf \u001b[39m=\u001b[39m TfidfVectorizer(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m, lowercase\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, tokenizer\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m text: text) \u001b[39m# , max_df=0.1\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(tfidf) \u001b[39m#ovo je iz nekog razloga prazan dokument\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m results \u001b[39m=\u001b[39m tfidf\u001b[39m.\u001b[39mfit_transform(df\u001b[39m.\u001b[39mdescription)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m result_dense \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39mtodense()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m tfidf_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(result_dense\u001b[39m.\u001b[39mtolist(), index\u001b[39m=\u001b[39mdf\u001b[39m.\u001b[39mindex, columns\u001b[39m=\u001b[39mtfidf\u001b[39m.\u001b[39mget_feature_names_out())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[1;32m   2120\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2121\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[1;32m   2122\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[1;32m   2123\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[1;32m   2124\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[1;32m   2125\u001b[0m )\n\u001b[0;32m-> 2126\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mfit_transform(raw_documents)\n\u001b[1;32m   2127\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[1;32m   2128\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2129\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1376\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1377\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1378\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1379\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1380\u001b[0m             )\n\u001b[1;32m   1381\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1383\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_count_vocab(raw_documents, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfixed_vocabulary_)\n\u001b[1;32m   1385\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1386\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1269\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1270\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1271\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1272\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:105\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m decoder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     doc \u001b[39m=\u001b[39m decoder(doc)\n\u001b[1;32m    106\u001b[0m \u001b[39mif\u001b[39;00m analyzer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     doc \u001b[39m=\u001b[39m analyzer(doc)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:238\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    235\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39mdecode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode_error)\n\u001b[1;32m    237\u001b[0m \u001b[39mif\u001b[39;00m doc \u001b[39mis\u001b[39;00m np\u001b[39m.\u001b[39mnan:\n\u001b[0;32m--> 238\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    239\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m     )\n\u001b[1;32m    242\u001b[0m \u001b[39mreturn\u001b[39;00m doc\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(input='content', lowercase=False, tokenizer=lambda text: text) # , max_df=0.1\n",
    "print(tfidf) #ovo je iz nekog razloga prazan dokument\n",
    "results = tfidf.fit_transform(df.description)\n",
    "result_dense = results.todense()\n",
    "tfidf_data = pd.DataFrame(result_dense.tolist(), index=df.index, columns=tfidf.get_feature_names_out())\n",
    "len(tfidf_data)\n",
    "Counter(df.loc[14, 'description'])['you']\n",
    "df.loc[[1, 2,9 , 12, 14], 'descr_len']\n",
    "tfidf_data.loc[tfidf_data.design > 0, ['design']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
