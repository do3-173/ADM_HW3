{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Engine\n",
    "\n",
    "Now, we want to create two different Search Engines that, given as input a query, return the courses that match the query.\n",
    "\n",
    "### 2.0 Preprocessing \n",
    "\n",
    "### 2.0.0)  Preprocessing the text\n",
    "\n",
    "First, you must pre-process all the information collected for each MSc by:\n",
    "\n",
    "1. Removing stopwords\n",
    "2. Removing punctuation\n",
    "3. Stemming\n",
    "4. Anything else you think it's needed\n",
    "   \n",
    "For this purpose, you can use the [`nltk library](https://www.nltk.org/).\n",
    "\n",
    "### 2.0.1) Preprocessing the fees column\n",
    "\n",
    "Moreover, we want the field ```fees``` to collect numeric information. As you will see, you scraped textual information for this attribute in the dataset: sketch whatever method you need (using regex, for example, to find currency symbol) to collect information and, in case of multiple information, retrieve only the highest fees. Finally, once you have collected numerical information, you likely will have different currencies: this can be chaotic, so let chatGPT guide you in the choice and deployment of an API to convert this column to a common currency of your choice (it can be USD, EUR or whatever you want). Ultimately, you will have a ```float``` column renamed ```fees (CHOSEN COMMON CURRENCY)```.\n",
    "\n",
    "### 2.1. Conjunctive query\n",
    "For the first version of the search engine, we narrowed our interest to the __description__ of each course. It means that you will evaluate queries only concerning the course's description.\n",
    "\n",
    "### 2.1.1) Create your index!\n",
    "\n",
    "Before building the index, \n",
    "* Create a file named `vocabulary`, in the format you prefer, that maps each word to an integer (`term_id`).\n",
    "\n",
    "Then, the first brick of your homework is to create the Inverted Index. It will be a dictionary in this format:\n",
    "\n",
    "```\n",
    "{\n",
    "term_id_1:[document_1, document_2, document_4],\n",
    "term_id_2:[document_1, document_3, document_5, document_6],\n",
    "...}\n",
    "```\n",
    "where _document\\_i_ is the *id* of a document that contains that specific word.\n",
    "\n",
    "__Hint:__ Since you do not want to compute the inverted index every time you use the Search Engine, it is worth thinking about storing it in a separate file and loading it in memory when needed.\n",
    "\n",
    "#### 2.1.2) Execute the query\n",
    "Given a query input by the user, for example:\n",
    "\n",
    "```\n",
    "advanced knowledge\n",
    "```\n",
    "\n",
    "The Search Engine is supposed to return a list of documents.\n",
    "\n",
    "##### What documents do we want?\n",
    "Since we are dealing with conjunctive queries (AND), each returned document should contain all the words in the query.\n",
    "The final output of the query must return, if present, the following information for each of the selected documents:\n",
    "\n",
    "* `courseName`\n",
    "* `universityName`\n",
    "* `description`\n",
    "* `URL`\n",
    "\n",
    "__Example Output__ for ```advanced knowledge```: (please note that our examples are made on a small batch of the full dataset)\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"img/output1.png\" width = 1000>\n",
    "</p>\n",
    "\n",
    "If everything works well in this step, you can go to the next point and make your Search Engine more complex and better at answering queries.\n",
    "\n",
    "\n",
    "### 2.2) Conjunctive query & Ranking score\n",
    "\n",
    "For the second search engine, given a query, we want to get the *top-k* (the choice of *k* it's up to you!) documents related to the query. In particular:\n",
    "\n",
    "* Find all the documents that contain all the words in the query.\n",
    "* Sort them by their similarity with the query.\n",
    "* Return in output *k* documents, or all the documents with non-zero similarity with the query when the results are less than _k_. You __must__ use a heap data structure (you can use Python libraries) for maintaining the *top-k* documents.\n",
    "\n",
    "To solve this task, you must use the *tfIdf* score and the _Cosine similarity_. The field to consider is still the `description`. Let's see how.\n",
    "\n",
    "\n",
    "#### 2.2.1) Inverted index\n",
    "Your second Inverted Index must be of this format:\n",
    "\n",
    "```\n",
    "{\n",
    "term_id_1:[(document1, tfIdf_{term,document1}), (document2, tfIdf_{term,document2}), (document4, tfIdf_{term,document4}), ...],\n",
    "term_id_2:[(document1, tfIdf_{term,document1}), (document3, tfIdf_{term,document3}), (document5, tfIdf_{term,document5}), (document6, tfIdf_{term,document6}), ...],\n",
    "...}\n",
    "```\n",
    "\n",
    "Practically, for each word, you want the list of documents in which it is contained and the relative *tfIdf* score.\n",
    "\n",
    "__Tip__: *TfIdf* values are invariant for the query. Due to this reason, you can precalculate and store them accordingly.\n",
    "\n",
    "#### 2.2.2) Execute the query\n",
    "\n",
    "In this new setting, given a query, you get the proper documents (i.e., those containing all the query's words) and sort them according to their similarity to the query. For this purpose, as the scoring function, we will use the Cosine Similarity concerning the *tfIdf* representations of the documents.\n",
    "\n",
    "Given a query input by the user, for example:\n",
    "```\n",
    "advanced knowledge\n",
    "```\n",
    "The search engine is supposed to return a list of documents, __ranked__ by their Cosine Similarity to the query entered in the input.\n",
    "\n",
    "More precisely, the output must contain:\n",
    "* `courseName`\n",
    "* `universityName`\n",
    "* `description`\n",
    "* `URL`\n",
    "* The similarity score of the documents with respect to the query (float value between 0 and 1)\n",
    "  \n",
    "__Example Output__ for ```advanced knowledge```:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"img/output2.png\" width = 1000>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the TSV data\n",
    "df = pd.read_csv(\n",
    "    \"TSV/course_1.tsv\", sep=\"\\t\", index_col=False\n",
    ")\n",
    "\n",
    "for i in range(2, 6001):\n",
    "    try:\n",
    "        df1 = pd.read_csv(\n",
    "            \"TSV/course_\" + str(i) + \".tsv\",\n",
    "            sep=\"\\t\",\n",
    "            index_col=False,\n",
    "        )\n",
    "        df1.index += i - 1\n",
    "        df = pd.concat([df, df1])\n",
    "    except Exception as e:\n",
    "        print(i)\n",
    "        print(\"Error: \", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/petraudovicic/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Removing stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def stopless(text):\n",
    "    if isinstance(text, str):\n",
    "        words = word_tokenize(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        return \" \".join(filtered_words)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "df = df.applymap(stopless)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/petraudovicic/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "#removing punctuation\n",
    "nltk.download('punkt')\n",
    "def punct(text):\n",
    "    if isinstance(text, str):\n",
    "        words = word_tokenize(text)\n",
    "        filtered_words = [word for word in words if word.lower() not in string.punctuation]\n",
    "        return \" \".join(filtered_words)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "df = df.applymap(punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/petraudovicic/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "ps = PorterStemmer()\n",
    "def stem(text):\n",
    "    if isinstance(text, str):\n",
    "        words = word_tokenize(text)\n",
    "        stemmed_words = [ps.stem(word) for word in words]\n",
    "        return \" \".join(stemmed_words)\n",
    "    else:\n",
    "        return text\n",
    "df = df.applymap(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fees</th>\n",
       "      <th>converted_fees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uk student full time £9450 2022/2023 academ ye...</td>\n",
       "      <td>18880.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>tuition fee uk student start full-tim studi 20...</td>\n",
       "      <td>11797.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>uk student integr pre-mast cours £10,500 msc c...</td>\n",
       "      <td>12015.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>full time home award fee £11500 home modul fee...</td>\n",
       "      <td>13159.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>€11,000</td>\n",
       "      <td>11000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5988</th>\n",
       "      <td>full time £18,000 part time £9,000</td>\n",
       "      <td>20597.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5989</th>\n",
       "      <td>full time £18,000 part time £9,000</td>\n",
       "      <td>20597.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5993</th>\n",
       "      <td>full time £14,100 part time £7,050</td>\n",
       "      <td>16134.630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5994</th>\n",
       "      <td>full time £18,000 part time £9,000</td>\n",
       "      <td>20597.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>full time £18,000 part time £9,000</td>\n",
       "      <td>20597.400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1228 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   fees  converted_fees\n",
       "0     uk student full time £9450 2022/2023 academ ye...       18880.950\n",
       "29    tuition fee uk student start full-tim studi 20...       11797.733\n",
       "49    uk student integr pre-mast cours £10,500 msc c...       12015.150\n",
       "50    full time home award fee £11500 home modul fee...       13159.450\n",
       "51                                              €11,000       11000.000\n",
       "...                                                 ...             ...\n",
       "5988                 full time £18,000 part time £9,000       20597.400\n",
       "5989                 full time £18,000 part time £9,000       20597.400\n",
       "5993                 full time £14,100 part time £7,050       16134.630\n",
       "5994                 full time £18,000 part time £9,000       20597.400\n",
       "5997                 full time £18,000 part time £9,000       20597.400\n",
       "\n",
       "[1228 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def convert_to_numeric(value):\n",
    "    value = re.sub(r'eur|sek|chf|gbp|rmb|jpy|qr|[£€]', '', value)\n",
    "    # Remove commas from the numeric part (e.g., for values like '1,000')\n",
    "    value = value.replace(',', '')\n",
    "    value=value.replace(' ', '')\n",
    "    return float(value)\n",
    "def find_fees(text):\n",
    "    if isinstance(text,str):\n",
    "        pattern = r'\\b\\d{4}/\\d{4}\\b'\n",
    "        text = re.sub(pattern, '', text)\n",
    "        pattern = r'\\b\\d{4}/\\d{2}\\b'\n",
    "        text = re.sub(pattern, '', text)\n",
    "        # Define the regular expression pattern for currency symbols and values\n",
    "        pattern = r'((eur|sek|chf|gbp|rmb|jpy|qr|[£€])\\s?\\d+(?:[.,\\s]\\d{3})*(?:[.,]\\d{2})?|\\d+(?:[.,\\s]\\d{3})*(?:[.,]\\d{2})?\\s?(eur|sek|chf|gbp|rmb|jpy|qr|[£€]))'\n",
    "        # Find all matches in the text\n",
    "        matches = re.findall(pattern, text)\n",
    "        # Convert values to numeric format and filter out non-euro values\n",
    "        numeric_values = []\n",
    "        exchange_rates = {'SEK': 0.08588, 'GBP': 1.1443, 'CHF': 1.03708, 'JPY': 0.00618, 'QR': 0.25672, \"RMB\": 0.12892}\n",
    "        for value in matches:\n",
    "            value_numeric = convert_to_numeric(value[0])\n",
    "            if (('€' or 'eur') in value):\n",
    "                numeric_values.append(value_numeric)\n",
    "            elif 'sek' in value and exchange_rates is not None and 'SEK' in exchange_rates:\n",
    "                numeric_values.append(value_numeric * exchange_rates['SEK'])\n",
    "            elif ('£' or 'gbp') in value and exchange_rates is not None and 'GBP' in exchange_rates:\n",
    "                numeric_values.append(value_numeric * exchange_rates['GBP'])\n",
    "            elif 'chf' in value and exchange_rates is not None and 'CHF' in exchange_rates:\n",
    "                numeric_values.append(value_numeric * exchange_rates['CHF'])\n",
    "            elif 'jpy' in value and exchange_rates is not None and 'JPY' in exchange_rates:\n",
    "                numeric_values.append(value_numeric * exchange_rates['JPY'])\n",
    "            elif 'qr' in value and exchange_rates is not None and 'QR' in exchange_rates:\n",
    "                numeric_values.append(value_numeric * exchange_rates['QR'])\n",
    "            else:\n",
    "                if ('qr' in value and exchange_rates is not None and 'QR' in exchange_rates):\n",
    "                    numeric_values.append(convert_to_numeric(value) * exchange_rates['RMB'])\n",
    "\n",
    "        if not numeric_values:\n",
    "            return None  # No euro values found\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "    # Find the largest value\n",
    "    max_value = max(numeric_values)\n",
    "\n",
    "    return max_value\n",
    "df['converted_fees'] = df['fees'].apply(find_fees)\n",
    "filtered_df = df.dropna(subset=['converted_fees'])[['fees','converted_fees']]\n",
    "filtered_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m vocabulary\u001b[39m=\u001b[39m\u001b[39mset\u001b[39m()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dataset\u001b[39m.\u001b[39mdescr_clean\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: [vocabulary\u001b[39m.\u001b[39madd(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m row])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m vocabulary\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m index \u001b[39m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "vocabulary=set()\n",
    "df.description.apply(lambda row: [vocabulary.add(word) for word in row])\n",
    "vocabulary\n",
    "index = {}\n",
    "unique_id = 1\n",
    "for word in list(vocabulary):\n",
    "  index[unique_id] = word\n",
    "  unique_id+=1\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'_io.TextIOWrapper' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file_path, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, newline\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m tsv_file:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     tsv_reader \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mreader(tsv_file, delimiter\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mprint\u001b[39m(inverted_index(\u001b[39m'\u001b[39m\u001b[39madvanced knowledge\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[1;32m/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb Cell 9\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m      d[i]\u001b[39m=\u001b[39m[]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m text\u001b[39m.\u001b[39msplit(): \u001b[39m#ovo se vjv ne moze ovako\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mif\u001b[39;00m ((i \u001b[39min\u001b[39;00m tsv_file[\u001b[39m4\u001b[39m]) \u001b[39mand\u001b[39;00m (tsv_file[\u001b[39m0\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m d\u001b[39m.\u001b[39mget(i, []))): \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m             d[i]\u001b[39m.\u001b[39mextend(tsv_file[\u001b[39m0\u001b[39m])   \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mreturn\u001b[39;00m d\n",
      "\u001b[0;31mTypeError\u001b[0m: '_io.TextIOWrapper' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def inverted_index(text):\n",
    "    d=dict({})\n",
    "    for i in text.split():\n",
    "         d[i]=[]\n",
    "    for i in text.split(): #ovo se vjv ne moze ovako\n",
    "        if ((i in tsv_file[4]) and (tsv_file[0] not in d.get(i, []))): \n",
    "                d[i].extend(tsv_file[0])   \n",
    "    return d\n",
    "\n",
    "directory_path = '/Users/petraudovicic/Desktop/adm/ADM_HW3/TSV'\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".tsv\"):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "        # Open the TSV file and read its contents using the csv module\n",
    "        with open(file_path, 'r', newline='', encoding='utf-8') as tsv_file:\n",
    "            tsv_reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "            print(inverted_index('advanced knowledge'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
