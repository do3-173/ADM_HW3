{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Engine\n",
    "\n",
    "Now, we want to create two different Search Engines that, given as input a query, return the courses that match the query.\n",
    "\n",
    "### 2.0 Preprocessing \n",
    "\n",
    "### 2.0.0)  Preprocessing the text\n",
    "\n",
    "First, you must pre-process all the information collected for each MSc by:\n",
    "\n",
    "1. Removing stopwords\n",
    "2. Removing punctuation\n",
    "3. Stemming\n",
    "4. Anything else you think it's needed\n",
    "   \n",
    "For this purpose, you can use the [`nltk library](https://www.nltk.org/).\n",
    "\n",
    "### 2.0.1) Preprocessing the fees column\n",
    "\n",
    "Moreover, we want the field ```fees``` to collect numeric information. As you will see, you scraped textual information for this attribute in the dataset: sketch whatever method you need (using regex, for example, to find currency symbol) to collect information and, in case of multiple information, retrieve only the highest fees. Finally, once you have collected numerical information, you likely will have different currencies: this can be chaotic, so let chatGPT guide you in the choice and deployment of an API to convert this column to a common currency of your choice (it can be USD, EUR or whatever you want). Ultimately, you will have a ```float``` column renamed ```fees (CHOSEN COMMON CURRENCY)```.\n",
    "\n",
    "### 2.1. Conjunctive query\n",
    "For the first version of the search engine, we narrowed our interest to the __description__ of each course. It means that you will evaluate queries only concerning the course's description.\n",
    "\n",
    "### 2.1.1) Create your index!\n",
    "\n",
    "Before building the index, \n",
    "* Create a file named `vocabulary`, in the format you prefer, that maps each word to an integer (`term_id`).\n",
    "\n",
    "Then, the first brick of your homework is to create the Inverted Index. It will be a dictionary in this format:\n",
    "\n",
    "```\n",
    "{\n",
    "term_id_1:[document_1, document_2, document_4],\n",
    "term_id_2:[document_1, document_3, document_5, document_6],\n",
    "...}\n",
    "```\n",
    "where _document\\_i_ is the *id* of a document that contains that specific word.\n",
    "\n",
    "__Hint:__ Since you do not want to compute the inverted index every time you use the Search Engine, it is worth thinking about storing it in a separate file and loading it in memory when needed.\n",
    "\n",
    "#### 2.1.2) Execute the query\n",
    "Given a query input by the user, for example:\n",
    "\n",
    "```\n",
    "advanced knowledge\n",
    "```\n",
    "\n",
    "The Search Engine is supposed to return a list of documents.\n",
    "\n",
    "##### What documents do we want?\n",
    "Since we are dealing with conjunctive queries (AND), each returned document should contain all the words in the query.\n",
    "The final output of the query must return, if present, the following information for each of the selected documents:\n",
    "\n",
    "* `courseName`\n",
    "* `universityName`\n",
    "* `description`\n",
    "* `URL`\n",
    "\n",
    "__Example Output__ for ```advanced knowledge```: (please note that our examples are made on a small batch of the full dataset)\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"img/output1.png\" width = 1000>\n",
    "</p>\n",
    "\n",
    "If everything works well in this step, you can go to the next point and make your Search Engine more complex and better at answering queries.\n",
    "\n",
    "\n",
    "### 2.2) Conjunctive query & Ranking score\n",
    "\n",
    "For the second search engine, given a query, we want to get the *top-k* (the choice of *k* it's up to you!) documents related to the query. In particular:\n",
    "\n",
    "* Find all the documents that contain all the words in the query.\n",
    "* Sort them by their similarity with the query.\n",
    "* Return in output *k* documents, or all the documents with non-zero similarity with the query when the results are less than _k_. You __must__ use a heap data structure (you can use Python libraries) for maintaining the *top-k* documents.\n",
    "\n",
    "To solve this task, you must use the *tfIdf* score and the _Cosine similarity_. The field to consider is still the `description`. Let's see how.\n",
    "\n",
    "\n",
    "#### 2.2.1) Inverted index\n",
    "Your second Inverted Index must be of this format:\n",
    "\n",
    "```\n",
    "{\n",
    "term_id_1:[(document1, tfIdf_{term,document1}), (document2, tfIdf_{term,document2}), (document4, tfIdf_{term,document4}), ...],\n",
    "term_id_2:[(document1, tfIdf_{term,document1}), (document3, tfIdf_{term,document3}), (document5, tfIdf_{term,document5}), (document6, tfIdf_{term,document6}), ...],\n",
    "...}\n",
    "```\n",
    "\n",
    "Practically, for each word, you want the list of documents in which it is contained and the relative *tfIdf* score.\n",
    "\n",
    "__Tip__: *TfIdf* values are invariant for the query. Due to this reason, you can precalculate and store them accordingly.\n",
    "\n",
    "#### 2.2.2) Execute the query\n",
    "\n",
    "In this new setting, given a query, you get the proper documents (i.e., those containing all the query's words) and sort them according to their similarity to the query. For this purpose, as the scoring function, we will use the Cosine Similarity concerning the *tfIdf* representations of the documents.\n",
    "\n",
    "Given a query input by the user, for example:\n",
    "```\n",
    "advanced knowledge\n",
    "```\n",
    "The search engine is supposed to return a list of documents, __ranked__ by their Cosine Similarity to the query entered in the input.\n",
    "\n",
    "More precisely, the output must contain:\n",
    "* `courseName`\n",
    "* `universityName`\n",
    "* `description`\n",
    "* `URL`\n",
    "* The similarity score of the documents with respect to the query (float value between 0 and 1)\n",
    "  \n",
    "__Example Output__ for ```advanced knowledge```:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"img/output2.png\" width = 1000>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the TSV data\n",
    "df = pd.read_csv(\n",
    "    \"TSV/course_1.tsv\", sep=\"\\t\", index_col=False\n",
    ")\n",
    "\n",
    "for i in range(2, 6001):\n",
    "    try:\n",
    "        df1 = pd.read_csv(\n",
    "            \"TSV/course_\" + str(i) + \".tsv\",\n",
    "            sep=\"\\t\",\n",
    "            index_col=False,\n",
    "        )\n",
    "        df1.index += i - 1\n",
    "        df = pd.concat([df, df1])\n",
    "    except Exception as e:\n",
    "        print(i)\n",
    "        print(\"Error: \", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/petraudovicic/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Removing stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def stopless(text):\n",
    "    if isinstance(text, str):\n",
    "        words = word_tokenize(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        return \" \".join(filtered_words)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "df = df.applymap(stopless)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/petraudovicic/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "#removing punctuation\n",
    "nltk.download('punkt')\n",
    "def punct(text):\n",
    "    if isinstance(text, str):\n",
    "        words = word_tokenize(text)\n",
    "        filtered_words = [word for word in words if word.lower() not in string.punctuation]\n",
    "        return \" \".join(filtered_words)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "df = df.applymap(punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/petraudovicic/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "ps = PorterStemmer()\n",
    "def stem(text):\n",
    "    if isinstance(text, str):\n",
    "        words = word_tokenize(text)\n",
    "        stemmed_words = [ps.stem(word) for word in words]\n",
    "        return \" \".join(stemmed_words)\n",
    "    else:\n",
    "        return text\n",
    "df = df.applymap(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X15sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     max_value \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(numeric_values)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X15sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m max_value\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X15sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mconverted_fees\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mfees\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(find_fees)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X15sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m df[[\u001b[39m'\u001b[39m\u001b[39mfees\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mconverted_fees\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39mapply()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mmap_infer(\n\u001b[1;32m   1077\u001b[0m             values,\n\u001b[1;32m   1078\u001b[0m             f,\n\u001b[1;32m   1079\u001b[0m             convert\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvert_dtype,\n\u001b[1;32m   1080\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m pattern \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m([£€$]\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms?\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+(?:[.,\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms]\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md\u001b[39m\u001b[39m{3}\u001b[39;00m\u001b[39m)*(?:[.,]\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md\u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m)?|\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+(?:[.,\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms]\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md\u001b[39m\u001b[39m{3}\u001b[39;00m\u001b[39m)*(?:[.,]\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md\u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m)?\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms?[£€$])\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Find all matches in the text\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m matches \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mfindall(pattern, text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Convert values to numeric format and filter out non-euro values\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petraudovicic/Desktop/adm/ADM_HW3/search_engine.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m numeric_values \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/re/__init__.py:216\u001b[0m, in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfindall\u001b[39m(pattern, string, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m    209\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a list of all non-overlapping matches in the string.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \n\u001b[1;32m    211\u001b[0m \u001b[39m    If one or more capturing groups are present in the pattern, return\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m \n\u001b[1;32m    215\u001b[0m \u001b[39m    Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\u001b[39m.\u001b[39mfindall(string)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object, got 'float'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def convert_to_numeric(value):\n",
    "    # Remove currency symbols and replace spaces with empty string, then convert to float\n",
    "    value = re.sub(r'[£€$,]', '', value.replace(' ', ''))\n",
    "    return float(value)\n",
    "\n",
    "def find_fees(text, exchange_rates=None):\n",
    "    # Define the regular expression pattern for currency symbols and values\n",
    "    pattern = r'([£€$]\\s?\\d+(?:[.,\\s]\\d{3})*(?:[.,]\\d{2})?|\\d+(?:[.,\\s]\\d{3})*(?:[.,]\\d{2})?\\s?[£€$])'\n",
    "\n",
    "    # Find all matches in the text\n",
    "    matches = re.findall(pattern, text)\n",
    "\n",
    "    # Convert values to numeric format and filter out non-euro values\n",
    "    numeric_values = []\n",
    "    for value in matches:\n",
    "        value_numeric = convert_to_numeric(value)\n",
    "        if '€' in value:\n",
    "            numeric_values.append(value_numeric)\n",
    "        elif '$' in value and exchange_rates is not None and 'USD' in exchange_rates:\n",
    "            numeric_values.append(value_numeric * exchange_rates['USD'])\n",
    "        elif '£' in value and exchange_rates is not None and 'GBP' in exchange_rates:\n",
    "            numeric_values.append(value_numeric * exchange_rates['GBP'])\n",
    "\n",
    "    if not numeric_values:\n",
    "        return None  # No euro values found\n",
    "\n",
    "    # Find the largest value\n",
    "    max_value = max(numeric_values)\n",
    "\n",
    "    return max_value\n",
    "\n",
    "df['converted_fees'] = df['fees'].apply(find_fees)\n",
    "df[['fees','converted_fees']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
